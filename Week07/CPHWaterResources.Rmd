---
title: 'Water Resources in Copenhagen during 20th century'
date: "March-2021 updated`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r setup, include=FALSE}
library(knitr)
library(rmdformats)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=TRUE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

This script visualizes the spatial component of the data accompanying the Spring 2021 course on the City: Between Culture and Nature, taught by Mikkel Thelle and Mikkel Høghøj. 
The course surveys the gradual appearance of private and public bathing facilities, toilets and communal hygienic resources in the city of Copenhagen during the 20th century.
By editing elements in this script, you can explore different aspects of past and present hygienic amenities in the capital of Denmark.  

# Before we start: data wrangling
First load the packages necessary for spatial data visualisation and analysis.
```{r libraries}
library(sf)
library(tidyverse)
library(spatstat)
library(googlesheets4)
library(leaflet)
```

Next, load your spatial data - polygons representing the suburbs of Copenhagen. 
```{r}
suburbs <- sf::st_read("data/bydel.shp", options = "ENCODING=WINDOWS-1252")
plot(suburbs$geometry) # always check if there is geometry when loading a shapefile
tail(suburbs) # make sure that there are attributes by looking at the tail of the data
#write_rds(suburbs, "data/CPHsuburbs.rds")

# Checking id column
suburbs$id

```
Next attach the attribute data. 
I read the data from a google sheet where my colleagues and I can edit it. You can load it from there if you have a googlesheets4 package installed, or use the wc.csv in the data folder
```{r read-wc}
# LOAD DATA FROM GOOGLEDRIVE
# Uncomment the lines below to read data from GDrive
# wc <- read_sheet("https://docs.google.com/spreadsheets/d/1iFvycp6M6bF8GBkGjA2Yde2yCIhiy5_slAkGF-RUF7w/edit#gid=0",
#                     col_types = "cnnnnnnnn") # here we hard code what we want (c = character, n = numeric)
# write_csv(wc, "data/wc.csv")

# OR LOAD FROM DATA FOLDER
wc <- read_csv("data/wc.csv")
wc
# NB! "Flats" is a proxy for the number of households in the suburb
# Now we have the spatial attributes 
```

Data on access to hygienic facilities and other water resources in Copenhagen now looks good and tidy, but its spatial resolution is better than the provided polygons (as in we have multiple rows that all fit within one suburb id). We therefore need to aggregate the data before we attach it to the spatial polygons
```{r}
# Now we group the data by year
# For each year we summarize by suburb_id
wcdata <- wc %>% 
  group_by(year, suburb_id) %>% 
  summarize(flats = sum(flats),
            bath = sum(bath),
            pct_bath = bath/flats*100, # percentage of household with access to bath
            wc_access=sum(wc_access),
            pct_wc= wc_access/flats*100,
            warmH20=sum(hot_water),
            pct_wH20=warmH20/flats*100,
            communal_wc = sum(wc_communal_ct),
            communal_bath = sum(bath_communal_ct))
wcdata  
# Now we have each suburb for each year and the aggregated data 

#write_rds(wcdata, "data/CPH_wcdata.rds")
```

## Join the data with the spatial representations
Now we can join the data with the spatial polygons
```{r merge data}
wc_spatial <- suburbs %>% 
  merge(wcdata, by.x= "id",by.y ="suburb_id") # we merge by the shared column which is called something different in wcdata and suburbs
wc_spatial

# Now we have our spatial data
```
```{r check names}
# Examining the columns we now have
names(wc_spatial)
```
Lets look at the data in a map.

# Put the data on the map

## Flats and water resources in 1950
```{r plot1950}
# Filter to one year only
wc1950 <- wc_spatial %>% 
  filter(year==1950)

# Load tmap
library(tmap)

# Plotting
tmap_mode(mode = "plot")
tm_shape(wc1950) +
  #tm_facets(by = "year")+
  tm_borders(col = "black",
             lwd = 1) +
  tm_polygons("flats", # we can also look at pct_bath
             style = "pretty") + # we could also have chosen "jenks" which gives us natural/messy breaks 
  tm_legend(legend.position= c("RIGHT", "TOP"))+
  tm_compass(position = c("RIGHT", "BOTTOM"),
             type = "rose", 
             size = 2) +
  tm_scale_bar(position = c("RIGHT", "BOTTOM"),
               breaks = c(0, 2, 4),
               text.size = 1) +
  tm_credits(position = c("RIGHT", "BOTTOM"),
             text = "Adela Sobotkova, 2021") +
  tm_layout(main.title = "Copenhagen Flats",
            legend.outside = FALSE)

# White = missing data (there was no data on Frederiksberg)
```

## Flats through time
```{r view-flats}
library(tmap)

tmap_options(limits = c(facets.view = 6)) # we want to view 6 years
tmap_mode(mode = "view") # switching from plotting to view which displays the data on top of a background just like leaflet does
tm_shape(wc_spatial) +
  tm_facets(by = "year")+ # by year 
  tm_polygons("flats",
             style = "pretty")+ # we could also have used "jenks"
  tm_layout(main.title = "Copenhagen Flats",
            legend.outside = TRUE)

# NB! Consider using mf par row instead of facetting in order to see changes over time more clearly

```
<br>
## Lets look at flats per square kilometer

```{r addsqkm}
wc_spatial <- wc_spatial %>% 
  mutate(area_km2 = areal_m2/1000000,
         flat_per_km = flats/area_km2)
```

```{r viewflats-per-km,eval = FALSE}
library(tmap)
tmap_options(limits = c(facets.view = 6))
tmap_mode(mode = "view" )
tm_shape(wc_spatial) +
  tm_facets(by = "year") +
  tm_polygons("flat_per_km",
              n=5,
             style = "quantile") #+
  # tm_layout(main.title = "Copenhagen Flats per sq km",
  #           legend.outside = TRUE)
```

<br>
## Access to toilets and baths, per suburb and sq kilometer

Lets calculate the baths and toilets available per square kilometer per each suburb
```{r view-pct-bath}
library(tmap)
tmap_options(limits = c(facets.view = 6))
tmap_mode(mode = "view" )
tm_shape(wc_spatial) +
  tm_facets(by = "year") +
  tm_polygons("pct_bath",
             style = "pretty", 
             title = "Percentage of flats with <br> access to bath") #+
  # tm_layout(main.title = "Percentage of flats with access to a bath", legend.outside = TRUE)
```

Now we look at percentage of households that have access to baths 
```{r view-pct-wc}
library(tmap)
tmap_options(limits = c(facets.view = 6))
tmap_mode(mode = "view")
tm_shape(wc_spatial) +
  tm_facets(by = "year") +
  tm_polygons("pct_wc",
             style = "pretty", 
             title = "% of flats with <br> access to WC") +
  tm_layout(main.title = "Percentage of flats with access to WC",
            legend.outside = TRUE)
```
<br>
<br>
## You can further recalculate the number of baths per sq kilometer

```{r bath-per-km}
wc_spatial <- wc_spatial %>% 
  mutate(bath_per_km = bath/area_km2,
         wc_per_km = wc_access/area_km2)

```

### ..or continue with communal resources and warm water


```{r}

```

<br>
<br>

# Access OSM data for Copenhagen and retrieve (whatever would be relevant?)

The [OpenStreetMap](https://wiki.openstreetmap.org/wiki/Map_features) contains free and open spatial data for physical features on the ground, with each features' type being define using [key:value pair tags](https://wiki.openstreetmap.org/wiki/Map_features).  Each tag describes a geographic attribute of the feature being shown by that specific node, way or relation. 

It is all encoded with a "key" and a "value" - if you can find what you are interested in, in the value column then you can use OpenStreetMap data. 

Use:

* `osmdata:opq()` to define the bounding box (area of interest) of the osm request
* `osmdata:add_osm_feature()` to define the key:value pairs you are looking for
* `osmdata:osmdata_sf()` to retrieve the osm data.

```{r extract-osm-data}
library(osmdata)

# Create a bounding box
bb  <- suburbs %>% st_transform(4326) %>% st_bbox()
plot(bb)
q <- opq(bbox = bb,timeout = 180) # set the query. Timeout = how long the query should operate for before it times out

# Now we define the features we are interested in
qa  <- add_osm_feature(q, key = 'amenity',value = 'public_bath')
#qb     <- add_osm_feature(q, key = 'amenity',value = 'drinking_water')
qc     <- add_osm_feature(q, key = 'amenity',value = 'shower')
qd     <- add_osm_feature(q, key = 'amenity',value = 'toilets')
#qe     <- add_osm_feature(q, key = 'amenity',value = 'water_point')

# Now we need to retrieve them
public_bath <- c(osmdata_sf(qa),
                 osmdata_sf(qc),
                 osmdata_sf(qd))
```

## Clean up OSM data
Use the following code to clean the results and project them in Danish UTM.

This code:

* removes the duplicated geometries thanks to `osmdata::unique_osmdata` (see the documentation for details)
* projects into WGC84 UTM32 (This is the Danish UTM)
* keeps the name attribute only
* computes the centroids for the baths stored as polygons
* Eventually, the baths outside our CPH suburbs are removed.
```{r osm-wrangle}
library(osmdata)

# We want to look at public bathing facilities 
bath_uniq <- unique_osmdata(public_bath)

# Filtering points and transforming
rpoint <- bath_uniq$osm_points %>% 
  filter(!is.na(amenity)) %>% 
  st_transform(32632) %>%
  dplyr::select(name) 

# Filtering polygons and transforming
rpoly  <- bath_uniq$osm_polygons %>% 
  st_transform(32632) %>% 
  dplyr::select(name)  %>% 
  st_centroid()

# Bind the points and polygons together
baths_osm <- rbind(rpoly,rpoint)   

# Intersecting because we want to narrow it down to onlu include the baths inside the suburbs
baths_osm <- st_intersection(baths_osm, st_transform(suburbs, 32632) %>% st_geometry() %>% st_union())
# 32632 is the Danish UTM zone (the standard projection for Denmark) 

# transform also historical baths 
baths_cph <- wc_spatial%>% 
  st_centroid() %>% 
  st_transform(32632) %>% 
  mutate(radius = sqrt(bath_per_km)) %>% 
  arrange(desc(bath_per_km))
```

## Display two maps side-by-side
Now, let's display the results in two synchronized `mapview` maps:

* one with bathing resources in suburbs
* another one with baths extracted from OSM.
* Use the `mapview::sync` function to display both maps side by side with synchronisation.

```{r mapview-sync}
# We are using mapview to display the results in synchronized maps 
library(mapview)
library(leafsync)

# Loading baths_osm
map_osm <-  mapview(baths_osm, map.types = "OpenStreetMap", 
        col.regions = "#940000", 
        label = as.character(suburbs$name), 
        color = "white", legend = FALSE, layer.name = "Baths in OSM",
        homebutton = FALSE, lwd = 0.5) 

#test map
mapview(baths_cph[,-3], map.types = "Stamen.TonerLite", cex="radius", legend=FALSE,
        col.regions="#217844", lwd=0, alpha=0.4)

map_cph <-  mapview(baths_cph[,-3], 
          map.types = "OpenStreetMap", 
        col.regions = "#940000", 
        color = "white", 
        cex = "bath_per_km",
        legend = TRUE, 
        layer.name = "Baths per sq km <br>in suburbs from 1970",
        homebutton = FALSE, lwd = 0.5) 

sync(map_osm, map_cph) # Here we can see where there used to be a bath and what it looks like today
```

## Improve the display
The synced map functionality is nice, but the comparison does not make much sense: OSM public bathrooms versus private bathing facilities. It might be better to combine the OSM data with the public bathhouse data that we had looked at previously in Leaflet.

We need to 

* load the data from googlespreadsheet
* filter out missing coordinates and convert to sf object
* project to WGS84 UTM 32

```{r get-hist-baths}
# Read from GoogleDrive
# baths <- read_sheet("https://docs.google.com/spreadsheets/d/15i17dqdsRYv6tdboZIlxTmhdcaN-JtgySMXIXwb5WfE/edit#gid=0",
#                     col_types = "ccnnncnnnc")
# write_rds(baths,"data/baths.rds")

# Or read from data folder
baths <- read_rds("data/baths.rds") # public bath dataset
names(baths)

hist_bathhouses <- baths %>% 
  dplyr::select(BathhouseName,Longitude,Latitude,Quality) %>% 
  filter(!is.na(Longitude)) %>% 
  st_as_sf(coords=c("Longitude", "Latitude"), crs = 4236) # convert to sf object

hist_baths <- st_transform(hist_bathhouses, crs=32632)

#test map
mapview(hist_baths, map.types = "Stamen.TonerLite",
        #cex="radius", legend=FALSE,
        col.regions="#217844", lwd=0, alpha=0.4)

```

Now, let's load this projected historical bathouse object in the synced map so we can compare the locations with OSM data.
```{r}
library(mapview)
library(leafsync)

map_osm <-  mapview(baths_osm, map.types = "OpenStreetMap", 
        col.regions = "#940000", 
        label = as.character(suburbs$name), 
        color = "white", legend = FALSE, layer.name = "Baths in OSM",
        homebutton = FALSE, lwd = 0.5) 

map_hist <-  mapview(hist_baths, 
          map.types = "OpenStreetMap", 
        col.regions = "#940000", 
        color = "white", 
       # cex = "bath_per_km",
        legend = TRUE, 
        layer.name = "Public bathhouses, early 20th century",
        homebutton = FALSE, lwd = 0.5) 

sync(map_osm,map_hist)
```
<br>
<br>

# Comparing two point patterns. How do we best do it? 

We have two patterns, historical and OSM data. Are they similar or dissimilar? How do the patterns of historical and current public bathhouses compare beyond a quick eyeball evaluation?

Here we might be able to use some statistical functions that contrast nearest neighbor distances or multi-distance clustering across the two groups.

We should first check the nature of data:  do both patterns represent  *completely mapped data* rather than *sampled data* (where the nature of sampling can affect the comparison)? If the former, one could use nearest neighbor, K-function or Monte Carlo reassignment.

For a tutorial on Kcross function, see Manny G in https://gis.stackexchange.com/questions/4484/comparing-two-spatial-point-patterns#4490

## Before we try some cross-functions, we need to wrangle 
But first we need to recast the baths as `ppp` object.
Note: st_union did not work as expected (it is multiplying the features), and so I did a workaround and combined the baths sf objects. En route I found nd this neat post on unioning using Danish municipalities https://gis.stackexchange.com/questions/278818/fastest-way-to-union-a-set-of-polygons-in-r

```{r spatstat}
library(spatstat)

# Prepare the ppp object

# Rebuild ppp from scratch via a combined sf object
# Extract coordinates:
st_coordinates(hist_baths)  # 21 coordinates (baths)
st_coordinates(baths_osm)   # 166 coordinates (baths)

# We combine them
combined <- data.frame(rbind(st_coordinates(hist_baths),
                  st_coordinates(baths_osm)))
# Now we have a dataframes of combined coordinates

# Now I am ssigning marks which need to be a factor
# We create a new column called "name" and we assign the marks to it 
# H = Historical
# O = Osm 
combined$name <- factor(c(rep("H",21), rep("O", 166))) 
# Now we have a dataframe called combined with coordinates and type of point in the name column

# Create an sf object out of the dataframe
b_c <- st_as_sf(combined, coords = c("X","Y"), crs = 32632) # projection = UTM32

# Convert into a marked ppp and confirm by plotting
b_ppp <- as.ppp(b_c) # as.ppp() on sf object it takes the bounding box as the default window if you do not specify a window explicitly
plot(split(b_ppp)) # splitting the ppp object allows us to see the spatial distribution of the baths
```
## Nearest Neighbour Cross Function and Simulation
We randomly reassign marks (H, O) within the combined point dataset and then calculate nearest neighbor between the randomly replaced marked points. Run the simulation 999 times

1. Run nearest neighbor
2. Run simulation with random labels in order to see what random nearest neighbor would look like

```{r nn-sim}
# Nearest neighbor simulation
nn.sim  <- vector() #create container for simulation data

# Create temporary object 
P.r <-  b_ppp

# Running the simulation 999 times 
for(i in 1:999){
  marks(P.r)  <-  sample(b_ppp$marks)  # Reassign labels at random, point locations don't change
  nn.sim[i]  <-  mean(nncross(split(P.r)$O,split(P.r)$H)$dist) # calculating nearest neighbor cross function between the Os and Hs and then take the distance and calculate the mean. This is done for every single simulation (i.e. 999 times)
}
```

### Compare NN - simulation results visually
```{r nn-hist}
# Now we look at the nearest neighnor histogram showing us the simulated nearest neighbor distances 
hist(nn.sim,breaks=30)
abline(v=mean(nncross(split(b_ppp)$O,split(b_ppp)$H)$dist),col="red")
# We can see that if we simulate the distances, we get much closer distances while when with the actual dataset the points are much more dispersed. Hence, in the actual dataset there are much more dispersion

# Grey bars: nearest neigbor distances wiht randomly assigned points for H and O marks. Hence, this allows us to see what the distribution of nearest neigbor mean distances would look like with random assiged points. 

# red line: the actual nearest neighbor distance for the dataset that we actually have. This indicates that hte dataset that we actually have is much higher than with random points.
```
### Compute empirical cumulative distribution
```{r nn-cumul}
nn.sim.ecdf  <-  ecdf(nn.sim)
```

### See how the original stat compares to the simulated distribution
```{r}
nn.sim.ecdf(mean(nncross(split(b_ppp)$O,split(b_ppp)$H)$dist)) 
```

## Ripley-K cross function

Ripley's-K cross function is a measure of how much the pattern of the points changes with distance. Hence, the function will tell us whtehr the points are clustered or dispersed and whether the pattern between the points is more or less clustered with distance. 
The cross-function looks at the relation between classes (in our case it looks at whether the historical baths are clustered near the modern baths)

Maybe we should look at the multi-scale approach to the bathhouses.
Check out J.Levente's  Ripley K'cross-function [blog](http://blog.jlevente.com/understanding-the-cross-k-function/) and [tutorial](https://github.com/jlevente/publications/tree/master/cross-k). 

```{r kcross}
# Set intervals for moving window (you don't have to)
rc <- seq(0, 3000, 100)

# Run the Kcross function (Kcross is for bivariate point pattern)
kcross <- Kcross(b_ppp, i="H",j="O", 
                 # r=rc,
                 correction='none') 
plot(kcross)
# Here we can see the K cross biagram. We get Ripleyøs K measured from H to O. 

# Red line: the random spatial pattern. We can see that they continue to be clustered as the distance increases - hence, the histoical baths continue to be near the modern baths.

# Black line: observations. Since it is above the red line then the point pattern is clustered. It continues to be above the red line as the distance increases. This means that the historical and modern baths are clustered - also when distance is increased.

# Above red line: more clustered than expcted from the random distribution
# Below red line: more dispersed than expected
# Close to red line: complete randomness

# Poisson is the main distribution used to model spatial randomness.
```
How to explain this chart? It seems that the OSM baths cluster around historical baths, or are attracted to them even at distances. Or in other words, the 'O' events are closer to 'H' events that we would expect under complete spatial randomness. 

How do we test for statistical significance? The question here is whether the H and O events are similarly clustered or not? Statistical difference can be tested with MC simulation with random labelling of points as O or H type (keeping original ratios) and computing the same cross K-function. The simulation mean and the established simulation envelopes tell us whether the observed *between-type* pattern is statistically significant or not.
```{r simulate-kross-env}
kmult <- envelope(b_ppp, fun=Kcross,
                  nsim=100, i="H", j="O", # from H to O
                  #r=rc, 
                  correction='none',
                  simulate=expression(rlabel(b_ppp)))  # Here we are replacing the labels to see if the two patterns are similarly clustered or dispersed at different scales. 

plot(kmult, main="Cross-K function")
# Now we see that the black line is below the red line, which means that the patterns are dispersed, whihc is weird, because before they were clustered. This might have to do with the scaling. 
# This might mean that the two point patterns are not SIMILARLY clustered

# Grey: confidence intervals.

```
An observed curve within the confidence envelopes means that no matter how we group the points into categories, the pattern we identified in the previous step (by checking on the observed and theoretical values) doesn’t change when randomly assigning events into categories. Here the curve falls outside of the confidence envelopes, meaning that there are differences between the point categories.
